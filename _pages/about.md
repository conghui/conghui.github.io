---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am currently a Research Director at SenseTime Inc., as well as a Research Scientist and PI at the Shanghai AI Laboratory. Prior to this, I worked at WeChat as a Senior Researcher, where I initiated and developed the high-performance graph computing framework, [Plato](https://github.com/Tencent/plato). Before joining WeChat, I earned my PhD degree (2013-2018) from the Department of Computer Science at Tsinghua University under the supervision of Prof. [Haohuan Fu](http://47.94.243.94/mediawiki/index.php/Haohuan_Fu), and my Bachelor's degree (2009-2013) from the Department of Software Engineering at Sun Yat-Sen University.

My research interests include High Performance Computing, Computer Vision, and Large Language Models. In 2017, I was honored with the [Gordon Bell Prize](https://awards.acm.org/bell) , which is the highest distinction in the high-performance computing application domain. Currently, I lead the [OpenDataLab](https://opendatalab.com/) and the [MinerU](https://github.com/opendatalab/MinerU) team, which aims to build an influential open dataset platform and toolkit that facilitates the development, analysis and research of Artificial General Intelligence (AGI). Additionally, I oversee a data team that prepare high quality data for [InternLM](https://github.com/InternLM) and [InternVL](https://github.com/OpenGVLab/InternVL).

At SenseTime and the Shanghai AI Laboratory, we are actively hiring PhDs, postdocs, interns, and full-time researchers. If you're interested in joining our team, please feel free to reach out to me via email.

# üî• News
- *2025.05*: &nbsp;üéâ [[1‚Ä†](https://arxiv.org/abs/2503.16212)][2‚Ä†][[3‚Ä†](https://arxiv.org/abs/2504.12322)][[4‚Ä†](https://arxiv.org/abs/2504.14194)][[5‚Ä†](https://arxiv.org/abs/2503.21500)][[6‚Ä†](https://arxiv.org/abs/2503.17439)][[7‚Ä†](https://arxiv.org/abs/2502.11501)][[8](https://arxiv.org/abs/2504.19093)][[9](https://arxiv.org/abs/2402.17645)][[10](https://arxiv.org/abs/2501.12273)][[11](https://arxiv.org/abs/2505.12212)] papers are accepted by ACL 2025.
- *2025.02*: &nbsp;üéâ [[1‚Ä†](https://arxiv.org/abs/2412.07626)][[2‚Ä†](https://arxiv.org/abs/2501.05510)][[3‚Ä†](https://arxiv.org/abs/2409.03643)][[4](https://arxiv.org/abs/2502.20653)][[5](https://cvpr.thecvf.com/virtual/2025/poster/33817)] papers are accepted by CVPR 2025.
- *2025.01*: &nbsp;üéâ [1] papers is accepted by NACCL 2025.
- *2025.01*: &nbsp;üéâ [[1‚Ä†](https://arxiv.org/abs/2410.09732)][[2‚Ä†](https://arxiv.org/abs/2406.08418)][[3‚Ä†](https://arxiv.org/abs/2409.16986)][[4‚Ä†](https://openreview.net/pdf?id=C25SgeXWjE)][[5](https://arxiv.org/abs/2310.05375)][[6](https://arxiv.org/abs/2412.11863)][[7](https://arxiv.org/abs/2410.17637)]papers are accepted by ICLR 2025.
- *2024.12*: &nbsp;üéâ [[1‚Ä†](https://arxiv.org/abs/2408.17267)][[2‚Ä†](https://arxiv.org/abs/2410.06913)][[3‚Ä†](https://arxiv.org/abs/2403.20213)] papers are accepted by AAAI 2025.
- *2024.09*: &nbsp;üéâ [[1](https://arxiv.org/pdf/2404.06512)] papers is accepted by NeurlPS 2024.
- *2024.09*: &nbsp;üéâ [[1](https://arxiv.org/pdf/2406.16554)][[2](https://arxiv.org/pdf/2402.13583)][[3](https://arxiv.org/pdf/2403.02127)] papers are accepted by EMNLP 2024.
- *2024.07*: &nbsp;üéâ [[1‚Ä†](https://arxiv.org/abs/2312.14232)][2‚Ä†](https://arxiv.org/pdf/2408.05475)[[3](https://arxiv.org/abs/2311.12793)][[4](https://arxiv.org/abs/2307.06281)] papers are accepted by ECCV 2024.
- *2024.05*: &nbsp;üéâ [[1](https://arxiv.org/abs/2402.05935)] paper is accepted by ICML 2024.
- *2024.05*: &nbsp;üéâ [[1‚Ä†](https://arxiv.org/abs/2403.14112)][[2](https://arxiv.org/abs/2403.07920)] papers are accepted by ACL 2024.
- *2024.03*: &nbsp; We release [Wanjuan-CC](https://opendatalab.com/OpenDataLab/WanJuanCC), a safe and high-quality Webtext dataset.
- *2024.02*: &nbsp;üéâ [[1‚Ä†](https://arxiv.org/abs/2404.04823)][[2](https://arxiv.org/abs/2404.02638)][[3](https://arxiv.org/abs/2311.17911)] papers are accepted by CVPR 2024.
- *2023.09*: &nbsp; We release [InternLM2](https://github.com/InternLM/InternLM). See [arXiv](https://arxiv.org/abs/2403.17297) for details.
- *2023.09*: &nbsp;üéâ [[1](https://arxiv.org/abs/2308.12714)] paper is accepted by AAAI 2024.
- *2023.08*: &nbsp; We release [Wanjuan 1.0](https://opendatalab.com/OpenDataLab/WanJuan1_dot_0), a large-scale multi-modal dataset for pretraining.
- *2023.06*: &nbsp; We release [InternLM](https://github.com/InternLM/InternLM). You can find technical report [here](https://github.com/InternLM/InternLM-techreport/blob/main/InternLM.pdf).
- *2022.03*: &nbsp; We launch [OpenDataLab](https://opendatalab.com/), an open data platform that enpowers AGI.

# üíª Projects
- [OpenDataLab](https://opendatalab.com/)[![](https://img.shields.io/github/stars/opendatalab?style=social)](https://github.com/opendatalab), an open platform that facilitates the development of AGI by sharing datasets and open-sourced tools. It hosts over 7700 datasets and provides 50+ million data retrieval services to over 40,000 developers.
- [MinerU ![](https://img.shields.io/github/stars/opendatalab/MinerU?style=social)](https://github.com/opendatalab/MinerU), a one-stop, open-sourced and high-quality data extraction tool that supports PDF, webpage and e-book extraction. It is widely used in RAG as well as in training LLMs. 
- [InternLM ![](https://img.shields.io/github/stars/InternLM/InternLM?style=social)](https://github.com/InternLM/InternLM), a series of 7B and 20B base and chat models, featuring outstanding reasoning capability, 1M context window and the ability to use tools.  
- [PDF-Extract-Kit ![](https://img.shields.io/github/stars/opendatalab/PDF-Extract-Kit?style=social)](https://github.com/opendatalab/PDF-Extract-Kit), a comprehensive toolkit for high-quality PDF content extraction library.

# üìù Publications ([Google Scholar](https://scholar.google.com/citations?user=PopTv7kAAAAJ))
For a full list, please refer to my [google scholar](https://scholar.google.com/citations?user=PopTv7kAAAAJ). (* Interns & Students, ‚Ä† Corresponding Authors) 

## üìö Large Language Model (LLM)
1. `ECCV 2024` [Mmbench: Is your multi-modal model an all-around player?](https://arxiv.org/pdf/2307.06281), Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, **Conghui He**, Ziwei Liu, Kai Chen, Dahua Lin
2. `ECCV 2024` [Sharegpt4v: Improving large multi-modal models with better captions](https://arxiv.org/pdf/2311.12793), Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, **Conghui He**, Jiaqi Wang, Feng Zhao, Dahua Lin
3. `ACL 2024` [Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations](https://arxiv.org/abs/2403.14112), Jiaxing Sun*, Weiquan Huang, Jiang Wu, Chenya Gu, Wei Li, Songyang Zhang, Hang Yan, and **Conghui He‚Ä†**.
4. `AAAI 2024` [Vigc: Visual instruction generation and correction](https://arxiv.org/abs/2308.12714), Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, **Conghui He‚Ä†**
5. `ECCV 2024` [Parrot Captions Teach CLIP to Spot Text](https://arxiv.org/abs/2312.14232), Yiqi Lin*, **Conghui He‚Ä†**, Alex Jinpeng Wang, Bin Wang, Weijia Li, Mike Zheng Shou
6. `CVPR 2023` [Omnicity: Omnipotent city understanding with multi-level and multi-view images](https://arxiv.org/abs/2208.00928), Weijia Li, Yawen Lai, Linning Xu, Yuanbo Xiangli, Jinhua Yu, **Conghui He‚Ä†**, Gui-Song Xia‚Ä†, and Dahua Lin. 

## üíª High Performance Computing (HPC)
1. `SC 2017` [18.9-Pflops nonlinear earthquake simulation on Sunway TaihuLight: enabling depiction of 18-Hz and 8-meter scenarios](https://ieeexplore.ieee.org/document/9926274),Haohuan Fu‚Ä†, **Conghui He‚Ä†**, Bingwei Chen, Zekun Yin, Zhenguo Zhang et al. (ACM Gordon Bell Prize Award)
2. `TC 2017` [A fully-pipelined hardware design for gaussian mixture models](https://ieeexplore.ieee.org/document/7938761/), **Conghui He**, Haohuan Fu, Ce Guo, Wayne Luk, Guangwen Yang
3. `BigData 2019` [Finding Mutual X at WeChat-Scale Social Network in Ten Minutes](https://ieeexplore.ieee.org/document/9005513), **Conghui He**, Shijie Sun, Benli Li, Xiaogang Tu, Donghai Yu
4. `FCCM 2017` [A Nanosecond-level Hybrid Table Design for Financial Market Data Generators](), Haohuan Fu, **Conghui He**, Wayne Luk, Weijia Li, and Guangwen Yang

# üéñ Honors and Awards
- *2023*, SenseTime Award (Sensetime's highest award, 1 team from 100 teams)
- *2021*, Outstanding Team Award at SenseTime (10 teams from 200 teams)
- *2019*, Tencent Technology Breakthrough Award - Gold Prize (highest technical award, 1 team from 50 teams)
- *2018*, Outstanding Graduate PhD Student Award
- *2017*, ACM Gordon Bell Prize (the highest award in the field of HPC applications)
- *2017*, National PhD Scholarship Ôºà1%Ôºâ
- *2013*, Global Champion of the IEEE-IBM Smarter Planet Challenge (Team Leader, 1/54)
- *2010*, National Scholarship Ôºà1%Ôºâ